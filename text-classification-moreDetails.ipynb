{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "#import advertools as adv\n",
    "import json\n",
    "import collections\n",
    "import re\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from pandas import ExcelWriter\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "from nltk.stem.snowball import SnowballStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "NLTK Downloader\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> l\n",
      "\n",
      "Packages:\n",
      "  [ ] abc................. Australian Broadcasting Commission 2006\n",
      "  [ ] alpino.............. Alpino Dutch Treebank\n",
      "  [ ] averaged_perceptron_tagger Averaged Perceptron Tagger\n",
      "  [ ] averaged_perceptron_tagger_ru Averaged Perceptron Tagger (Russian)\n",
      "  [ ] basque_grammars..... Grammars for Basque\n",
      "  [ ] biocreative_ppi..... BioCreAtIvE (Critical Assessment of Information\n",
      "                           Extraction Systems in Biology)\n",
      "  [ ] bllip_wsj_no_aux.... BLLIP Parser: WSJ Model\n",
      "  [ ] book_grammars....... Grammars from NLTK Book\n",
      "  [ ] brown............... Brown Corpus\n",
      "  [ ] brown_tei........... Brown Corpus (TEI XML Version)\n",
      "  [ ] cess_cat............ CESS-CAT Treebank\n",
      "  [ ] cess_esp............ CESS-ESP Treebank\n",
      "  [ ] chat80.............. Chat-80 Data Files\n",
      "  [ ] city_database....... City Database\n",
      "  [ ] cmudict............. The Carnegie Mellon Pronouncing Dictionary (0.6)\n",
      "  [ ] comparative_sentences Comparative Sentence Dataset\n",
      "  [ ] comtrans............ ComTrans Corpus Sample\n",
      "  [ ] conll2000........... CONLL 2000 Chunking Corpus\n",
      "  [ ] conll2002........... CONLL 2002 Named Entity Recognition Corpus\n",
      "Hit Enter to continue: stopwords\n",
      "  [ ] conll2007........... Dependency Treebanks from CoNLL 2007 (Catalan\n",
      "                           and Basque Subset)\n",
      "  [ ] crubadan............ Crubadan Corpus\n",
      "  [ ] dependency_treebank. Dependency Parsed Treebank\n",
      "  [ ] dolch............... Dolch Word List\n",
      "  [ ] europarl_raw........ Sample European Parliament Proceedings Parallel\n",
      "                           Corpus\n",
      "  [ ] floresta............ Portuguese Treebank\n",
      "  [ ] framenet_v15........ FrameNet 1.5\n",
      "  [ ] framenet_v17........ FrameNet 1.7\n",
      "  [ ] gazetteers.......... Gazeteer Lists\n",
      "  [ ] genesis............. Genesis Corpus\n",
      "  [ ] gutenberg........... Project Gutenberg Selections\n",
      "  [ ] ieer................ NIST IE-ER DATA SAMPLE\n",
      "  [ ] inaugural........... C-Span Inaugural Address Corpus\n",
      "  [ ] indian.............. Indian Language POS-Tagged Corpus\n",
      "  [ ] jeita............... JEITA Public Morphologically Tagged Corpus (in\n",
      "                           ChaSen format)\n",
      "  [ ] kimmo............... PC-KIMMO Data Files\n",
      "  [ ] knbc................ KNB Corpus (Annotated blog corpus)\n",
      "  [ ] large_grammars...... Large context-free and feature-based grammars\n",
      "                           for parser comparison\n",
      "Hit Enter to continue: \n",
      "  [ ] lin_thesaurus....... Lin's Dependency Thesaurus\n",
      "  [ ] mac_morpho.......... MAC-MORPHO: Brazilian Portuguese news text with\n",
      "                           part-of-speech tags\n",
      "  [ ] machado............. Machado de Assis -- Obra Completa\n",
      "  [ ] masc_tagged......... MASC Tagged Corpus\n",
      "  [ ] maxent_ne_chunker... ACE Named Entity Chunker (Maximum entropy)\n",
      "  [ ] maxent_treebank_pos_tagger Treebank Part of Speech Tagger (Maximum entropy)\n",
      "  [ ] moses_sample........ Moses Sample Models\n",
      "  [ ] movie_reviews....... Sentiment Polarity Dataset Version 2.0\n",
      "  [ ] mte_teip5........... MULTEXT-East 1984 annotated corpus 4.0\n",
      "  [ ] mwa_ppdb............ The monolingual word aligner (Sultan et al.\n",
      "                           2015) subset of the Paraphrase Database.\n",
      "  [ ] names............... Names Corpus, Version 1.3 (1994-03-29)\n",
      "  [ ] nombank.1.0......... NomBank Corpus 1.0\n",
      "  [ ] nonbreaking_prefixes Non-Breaking Prefixes (Moses Decoder)\n",
      "  [ ] nps_chat............ NPS Chat\n",
      "  [ ] omw................. Open Multilingual Wordnet\n",
      "  [ ] opinion_lexicon..... Opinion Lexicon\n",
      "  [ ] panlex_swadesh...... PanLex Swadesh Corpora\n",
      "  [ ] paradigms........... Paradigm Corpus\n",
      "  [ ] pe08................ Cross-Framework and Cross-Domain Parser\n",
      "                           Evaluation Shared Task\n",
      "Hit Enter to continue: \n",
      "  [ ] perluniprops........ perluniprops: Index of Unicode Version 7.0.0\n",
      "                           character properties in Perl\n",
      "  [ ] pil................. The Patient Information Leaflet (PIL) Corpus\n",
      "  [ ] pl196x.............. Polish language of the XX century sixties\n",
      "  [ ] porter_test......... Porter Stemmer Test Files\n",
      "  [ ] ppattach............ Prepositional Phrase Attachment Corpus\n",
      "  [ ] problem_reports..... Problem Report Corpus\n",
      "  [ ] product_reviews_1... Product Reviews (5 Products)\n",
      "  [ ] product_reviews_2... Product Reviews (9 Products)\n",
      "  [ ] propbank............ Proposition Bank Corpus 1.0\n",
      "  [ ] pros_cons........... Pros and Cons\n",
      "  [ ] ptb................. Penn Treebank\n",
      "  [ ] punkt............... Punkt Tokenizer Models\n",
      "  [ ] qc.................. Experimental Data for Question Classification\n",
      "  [ ] reuters............. The Reuters-21578 benchmark corpus, ApteMod\n",
      "                           version\n",
      "  [ ] rslp................ RSLP Stemmer (Removedor de Sufixos da Lingua\n",
      "                           Portuguesa)\n",
      "  [ ] rte................. PASCAL RTE Challenges 1, 2, and 3\n",
      "  [ ] sample_grammars..... Sample Grammars\n",
      "  [ ] semcor.............. SemCor 3.0\n",
      "Hit Enter to continue: \n",
      "  [ ] senseval............ SENSEVAL 2 Corpus: Sense Tagged Text\n",
      "  [ ] sentence_polarity... Sentence Polarity Dataset v1.0\n",
      "  [ ] sentiwordnet........ SentiWordNet\n",
      "  [ ] shakespeare......... Shakespeare XML Corpus Sample\n",
      "  [ ] sinica_treebank..... Sinica Treebank Corpus Sample\n",
      "  [ ] smultron............ SMULTRON Corpus Sample\n",
      "  [ ] snowball_data....... Snowball Data\n",
      "  [ ] spanish_grammars.... Grammars for Spanish\n",
      "  [ ] state_union......... C-Span State of the Union Address Corpus\n",
      "  [ ] stopwords........... Stopwords Corpus\n",
      "  [ ] subjectivity........ Subjectivity Dataset v1.0\n",
      "  [ ] swadesh............. Swadesh Wordlists\n",
      "  [ ] switchboard......... Switchboard Corpus Sample\n",
      "  [ ] tagsets............. Help on Tagsets\n",
      "  [ ] timit............... TIMIT Corpus Sample\n",
      "  [ ] toolbox............. Toolbox Sample Files\n",
      "  [ ] treebank............ Penn Treebank Sample\n",
      "  [ ] twitter_samples..... Twitter Samples\n",
      "  [ ] udhr2............... Universal Declaration of Human Rights Corpus\n",
      "                           (Unicode Version)\n",
      "  [ ] udhr................ Universal Declaration of Human Rights Corpus\n",
      "Hit Enter to continue: d\n",
      "  [ ] unicode_samples..... Unicode Samples\n",
      "  [ ] universal_tagset.... Mappings to the Universal Part-of-Speech Tagset\n",
      "  [ ] universal_treebanks_v20 Universal Treebanks Version 2.0\n",
      "  [ ] vader_lexicon....... VADER Sentiment Lexicon\n",
      "  [ ] verbnet3............ VerbNet Lexicon, Version 3.3\n",
      "  [ ] verbnet............. VerbNet Lexicon, Version 2.1\n",
      "  [ ] webtext............. Web Text Corpus\n",
      "  [ ] wmt15_eval.......... Evaluation data from WMT15\n",
      "  [ ] word2vec_sample..... Word2Vec Sample\n",
      "  [ ] wordnet............. WordNet\n",
      "  [ ] wordnet_ic.......... WordNet-InfoContent\n",
      "  [ ] words............... Word Lists\n",
      "  [ ] ycoe................ York-Toronto-Helsinki Parsed Corpus of Old\n",
      "                           English Prose\n",
      "\n",
      "Collections:\n",
      "  [ ] all-corpora......... All the corpora\n",
      "  [ ] all-nltk............ All packages available on nltk_data gh-pages\n",
      "                           branch\n",
      "  [ ] all................. All packages\n",
      "  [ ] book................ Everything used in the NLTK Book\n",
      "  [ ] popular............. Popular packages\n",
      "Hit Enter to continue: d stopwords\n",
      "  [ ] tests............... Packages for running tests\n",
      "  [ ] third-party......... Third-party data packages\n",
      "\n",
      "([*] marks installed packages)\n",
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> d\n",
      "\n",
      "Download which package (l=list; x=cancel)?\n",
      "  Identifier> stopwords\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "    Downloading package stopwords to\n",
      "        C:\\Users\\svr\\AppData\\Roaming\\nltk_data...\n",
      "      Unzipping corpora\\stopwords.zip.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "---------------------------------------------------------------------------\n",
      "    d) Download   l) List    u) Update   c) Config   h) Help   q) Quit\n",
      "---------------------------------------------------------------------------\n",
      "Downloader> q\n"
     ]
    }
   ],
   "source": [
    "nltk.download_shell()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = []\n",
    "with open(\".\\\\english10k_tweets.json\",encoding='utf-8') as f:\n",
    "    for line in f:\n",
    "        data.append(json.loads(line))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "df=pd.DataFrame(columns=['tweet_id','tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [tweet_id, tweet_text]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "for line in data:\n",
    "    if \"extended_tweet\" in line:\n",
    "        df = df.append({'tweet_id': line['id'],\n",
    "            'tweet_text':line[\"extended_tweet\"]['full_text']         \n",
    "                       }, ignore_index=True)        \n",
    "    else:\n",
    "        df = df.append({'tweet_id': line['id'],\n",
    "            'tweet_text':line['text']           \n",
    "                       }, ignore_index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.set_index('tweet_id',inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We'll use read_csv and make note of the sep argument, we can also specify the desired column names by passing in a list of names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_lable = pd.read_csv(\"en10k_markup.txt\",sep=' ',index_col=1,names=[\"lable\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>9998.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.225145</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.417699</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             lable\n",
       "count  9998.000000\n",
       "mean      0.225145\n",
       "std       0.417699\n",
       "min       0.000000\n",
       "25%       0.000000\n",
       "50%       0.000000\n",
       "75%       0.000000\n",
       "max       1.000000"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lable.describe()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_final = pd.merge(df,df_lable, how='left',left_index=True,right_index=True)\n",
    "df_final.fillna(0,inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>815331870015627265</th>\n",
       "      <td>Wind 1,4 m/s ENE. Barometer 1003,2 hPa, Fallin...</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332374909222912</th>\n",
       "      <td>@pasdcheval i did it</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332387496357889</th>\n",
       "      <td>Happy New Year 2017! https://t.co/UFob5hzO5o</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332527254732800</th>\n",
       "      <td>Happy new Year from sweden</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332597886840834</th>\n",
       "      <td>Happy New Years!!!! 💫</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           tweet_text  lable\n",
       "tweet_id                                                                    \n",
       "815331870015627265  Wind 1,4 m/s ENE. Barometer 1003,2 hPa, Fallin...    1.0\n",
       "815332374909222912                               @pasdcheval i did it    0.0\n",
       "815332387496357889       Happy New Year 2017! https://t.co/UFob5hzO5o    0.0\n",
       "815332527254732800                         Happy new Year from sweden    0.0\n",
       "815332597886840834                              Happy New Years!!!! 💫    0.0"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.head()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exploratory data analysis of some data visulization and statistical "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exploratory Data Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>lable</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>10000.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.225100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.417669</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              lable\n",
       "count  10000.000000\n",
       "mean       0.225100\n",
       "std        0.417669\n",
       "min        0.000000\n",
       "25%        0.000000\n",
       "50%        0.000000\n",
       "75%        0.000000\n",
       "max        1.000000"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.describe() # to describe what's going on "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe thead tr:last-of-type th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th colspan=\"4\" halign=\"left\">tweet_text</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th></th>\n",
       "      <th>count</th>\n",
       "      <th>unique</th>\n",
       "      <th>top</th>\n",
       "      <th>freq</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>lable</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0.0</th>\n",
       "      <td>7749</td>\n",
       "      <td>7748</td>\n",
       "      <td>Anyone awake?</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1.0</th>\n",
       "      <td>2251</td>\n",
       "      <td>2241</td>\n",
       "      <td>We're #hiring! Read about our latest #job open...</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      tweet_text                                                            \\\n",
       "           count unique                                                top   \n",
       "lable                                                                        \n",
       "0.0         7749   7748                                      Anyone awake?   \n",
       "1.0         2251   2241  We're #hiring! Read about our latest #job open...   \n",
       "\n",
       "            \n",
       "      freq  \n",
       "lable       \n",
       "0.0      2  \n",
       "1.0      3  "
      ]
     },
     "execution_count": 108,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final.groupby('lable').describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>tweet_text</th>\n",
       "      <th>lable</th>\n",
       "      <th>length</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>tweet_id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>815331870015627265</th>\n",
       "      <td>Wind 1,4 m/s ENE. Barometer 1003,2 hPa, Fallin...</td>\n",
       "      <td>1.0</td>\n",
       "      <td>100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332374909222912</th>\n",
       "      <td>@pasdcheval i did it</td>\n",
       "      <td>0.0</td>\n",
       "      <td>20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332387496357889</th>\n",
       "      <td>Happy New Year 2017! https://t.co/UFob5hzO5o</td>\n",
       "      <td>0.0</td>\n",
       "      <td>44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332527254732800</th>\n",
       "      <td>Happy new Year from sweden</td>\n",
       "      <td>0.0</td>\n",
       "      <td>26</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>815332597886840834</th>\n",
       "      <td>Happy New Years!!!! 💫</td>\n",
       "      <td>0.0</td>\n",
       "      <td>21</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                           tweet_text  lable  \\\n",
       "tweet_id                                                                       \n",
       "815331870015627265  Wind 1,4 m/s ENE. Barometer 1003,2 hPa, Fallin...    1.0   \n",
       "815332374909222912                               @pasdcheval i did it    0.0   \n",
       "815332387496357889       Happy New Year 2017! https://t.co/UFob5hzO5o    0.0   \n",
       "815332527254732800                         Happy new Year from sweden    0.0   \n",
       "815332597886840834                              Happy New Years!!!! 💫    0.0   \n",
       "\n",
       "                    length  \n",
       "tweet_id                    \n",
       "815331870015627265     100  \n",
       "815332374909222912      20  \n",
       "815332387496357889      44  \n",
       "815332527254732800      26  \n",
       "815332597886840834      21  "
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['length'] = df_final['tweet_text'].apply(len)\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save dataframe in Excel sheet "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF TO EXCEL\n",
    "\n",
    "writer = ExcelWriter('PythonLastSaeideh.xlsx')\n",
    "df_final.to_excel(writer,'Sheet5')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Visualization "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.axes._subplots.AxesSubplot at 0x1f2f3f80390>"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "df_final['length'].plot(bins=50, kind='hist') \n",
    "# here we can see the distribution of the tweet text length"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    10000.000000\n",
       "mean        87.228300\n",
       "std         37.315531\n",
       "min          4.000000\n",
       "25%         56.000000\n",
       "50%         92.000000\n",
       "75%        115.000000\n",
       "max        190.000000\n",
       "Name: length, dtype: float64"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final['length'].describe() # longest text tweet is 190 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'@Spring__Dew @SandyAvila37 Hey 👋I have been a BEASTIES since 2013 When I saw the first \"Beauty and the Beast\"in DanishTV 📺and never seen anything like #BATB #SaveBatB https://t.co/szGrnlop0N'"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_final[df_final['length'] == 190]['tweet_text'].iloc[0] # Masking to see which row have longest text tweet\n",
    "# by writing .iloc[0] we can have entire text tweet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([<matplotlib.axes._subplots.AxesSubplot object at 0x000001F2F3F89588>,\n",
       "       <matplotlib.axes._subplots.AxesSubplot object at 0x000001F2F4051198>],\n",
       "      dtype=object)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x288 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# make histogrm of tweet-text (0=HGT, 1=AGT)\n",
    "# by basic data visulization we discover that HGT tweet text tend to have more character\n",
    "df_final.hist(column='length', by='lable', bins=50,figsize=(12,4))\n",
    "# Conclusion: tweet-text which are HGT tend to be longer than text-tweet that are AGT."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['i',\n",
       " 'me',\n",
       " 'my',\n",
       " 'myself',\n",
       " 'we',\n",
       " 'our',\n",
       " 'ours',\n",
       " 'ourselves',\n",
       " 'you',\n",
       " \"you're\",\n",
       " \"you've\",\n",
       " \"you'll\",\n",
       " \"you'd\",\n",
       " 'your',\n",
       " 'yours',\n",
       " 'yourself',\n",
       " 'yourselves',\n",
       " 'he',\n",
       " 'him',\n",
       " 'his']"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk.corpus import stopwords\n",
    "stopwords.words('english')[0:20] # Show some stop words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Porter stemmer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer = PorterStemmer(mode=\"MARTIN_EXTENSIONS\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "#stemmer.stem('swimming')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Snowball stemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [],
   "source": [
    "stemmer = SnowballStemmer(\"english\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'swim'"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#stemmer.stem('swimming')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 217,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_tweet(tweet):\n",
    "    tweet = re.sub('http\\S+\\s*', 'xurlx', tweet)  \n",
    "    tweet = re.sub('\\d[\\d,]*\\d|\\d+', 'xnumberx', tweet)  \n",
    "    tweet = re.sub('#\\S+', 'xhashtagx', tweet) \n",
    "    tweet = re.sub('@\\S+', 'xuserx', tweet) \n",
    "    tweet = re.sub('[%s]' % re.escape(\"\"\"!\"#$%&'()*+,-./:;<=>?@[\\]^_`{|}~\"\"\"), ' ', tweet)\n",
    "    tweet = re.sub('\\s+', ' ', tweet)  \n",
    "#     tweet = re.sub('\\s+', tweet.lower(), tweet)\n",
    "    return map(lambda x:stemmer.stem(x),tweet.split())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id\n",
       "815331870015627265    [wind, xnumberx, m, s, ene, baromet, xnumberx,...\n",
       "815332374909222912                                 [xuserx, i, did, it]\n",
       "815332387496357889                  [happi, new, year, xnumberx, xurlx]\n",
       "815332527254732800                     [happi, new, year, from, sweden]\n",
       "815332597886840834                                [happi, new, year, 💫]\n",
       "815333076070965249     [xuserx, xuserx, he, is, a, hunk, papi, nowaday]\n",
       "815333369751990276                           [xuserx, happi, new, year]\n",
       "815333438374998016    [xuserx, happi, new, year, and, happi, xnumber...\n",
       "815333549289209856    [not, onli, have, the, firework, been, go, off...\n",
       "815333756932333569    [i, know, it, probabl, didn, t, come, out, in,...\n",
       "815333875077566465              [when, you, p, r, o, g, r, a, m, xurlx]\n",
       "815333969801728001                     [happi, new, year, bitch, xurlx]\n",
       "815334054790893568                 [xuserx, same, to, you, amp, famili]\n",
       "815334109656522753    [guess, who, been, on, stage, xuserx, xhashtag...\n",
       "815334157626834944                         [happi, new, year, 😍, xurlx]\n",
       "815334231454994432         [happi, new, year, ya, ll, 🎇, xuserx, xurlx]\n",
       "815334667171794944    [xuserx, same, i, saw, justin, for, the, first...\n",
       "815335361538883584    [gott, nytt, xhashtagx, imperi, ftw, drink, a,...\n",
       "815335396745900032    [wind, xnumberx, m, s, nne, baromet, xnumberx,...\n",
       "815335415708192768    [xnumberx, user, and, xnumberx, tweet, xnumber...\n",
       "815335418652540928    [xnumberx, verifi, account, help, to, turn, go...\n",
       "815335693539045376    [✨💥, happi, new, year, everyon, ☄️💥, varatunve...\n",
       "815335831011467265    [xuserx, the, last, thing, i, said, in, xnumbe...\n",
       "815336030014414848                               [happi, new, year, 😚🎉]\n",
       "815336278938030080      [sista, godsaken, my, home, tvxnumberxb, xurlx]\n",
       "815336343941300228    [may, the, new, year, bring, lot, of, happi, g...\n",
       "815336583033298944    [new, year, in, iceland, with, xuserx, xuserx,...\n",
       "815337557089193990    [the, firework, here, in, malmö, at, midnight,...\n",
       "815337887491379200    [♫, she, will, be, love, ♪, by, xuserx, xurlxx...\n",
       "815337982265880577    [happi, new, year, 🎉, hope, xnumberx, will, be...\n",
       "                                            ...                        \n",
       "815347228256641024                   [happi, new, year, to, the, snail]\n",
       "815347330073432064    [that, foggi, feel, after, all, the, firework,...\n",
       "815347466325344257    [happi, new, year, dear, xuserx, may, it, be, ...\n",
       "815347615932022784    [happi, new, year, everyon, may, the, next, ye...\n",
       "815348149074137088    [happi, the, red, won, the, barclay, this, yea...\n",
       "815348356843274240      [big, ben, strike, xnumberx, in, london, xurlx]\n",
       "815348407074164736    [hello, xnumberx, i, got, a, small, request, p...\n",
       "815348480445186048    [wind, xnumberx, m, s, e, baromet, xnumberx, h...\n",
       "815348800604737536    [hey, xuserx, do, anoth, collab, with, xuserx,...\n",
       "815348987343622145    [xuserx, i, don, t, drink, alcohol, anyway, 😁,...\n",
       "815349644398067712    [we, re, top, of, the, leeeeeeaaaaaguuuuu, hap...\n",
       "815349656842629120    [xhashtagx, from, xhashtagx, 🇮🇸, xhashtagx, xh...\n",
       "815349991447416832    [wind, xnumberx, m, s, sse, baromet, xnumberx,...\n",
       "815350082346319872    [happi, new, year, my, peopl, from, the, euro,...\n",
       "815350518201585664    [my, own, xhashtagx, 📸, xhashtagx, in, may, my...\n",
       "815350789266882560    [alway, look, forward, new, year, new, begin, ...\n",
       "815351250141216768    [wind, xnumberx, m, s, se, baromet, xnumberx, ...\n",
       "815351300611313664                            [evergreen, tweet, xurlx]\n",
       "815351806012325889    [happi, new, year, my, bad, video, of, ppl, se...\n",
       "815351914208645120                        [one, word, happi, new, year]\n",
       "815352237379751936    [xuserx, xuserx, happi, new, year, to, my, ene...\n",
       "815352504498196480    [wind, xnumberx, m, s, w, baromet, xnumberx, h...\n",
       "815354002489704451    [im, turn, xnumberx, in, xnumberx, day, omfg, ...\n",
       "815354225819525121    [xnumberx, sad, caus, more, road, accid, than,...\n",
       "815354235349069825    [♫, mechanix, live, at, the, web, theatr, phoe...\n",
       "815354290646740992    [he, look, like, a, thug, with, his, baggi, pa...\n",
       "815355021353189377    [wind, xnumberx, m, s, e, baromet, xnumberx, h...\n",
       "815355195559473152    [xnumberx, head, a, footbal, can, reduc, your,...\n",
       "815355656186302464                                    [xuserx, haproxi]\n",
       "815356161172111360    [xuserx, thank, and, the, same, to, you, nicol...\n",
       "Name: tweet_text, Length: 100, dtype: object"
      ]
     },
     "execution_count": 252,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['tweet_text'].head(100).apply(clean_tweet).apply(list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we create Bag of Words transformer and to see how many time one specific word counts show up\n",
    "# we can also use .transform method on our BOW or transformed object and transfer the entire data frame of tweet text "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 219,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "11021\n"
     ]
    }
   ],
   "source": [
    "# Might take awhile...\n",
    "# the object we created\n",
    "bow_transformer = CountVectorizer(analyzer=clean_tweet).fit(df['tweet_text'])\n",
    "#bow_transformer = CountVectorizer(analyzer=clean_tweet).fit(df['tweet_text'].apply(clean_tweet).apply(list))\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#we can take one sample tweet text and get BOW counts as a vector putting "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 220,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wind 1,4 m/s ENE. Barometer 1003,2 hPa, Falling. Temperature 7,6 °C. Rain today 0,0 mm. Humidity 99%\n"
     ]
    }
   ],
   "source": [
    "message4 = df.loc[815331870015627265,'tweet_text']\n",
    "print(message4)\n",
    "# print(df.loc[[0]])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 725)\t1\n",
      "  (0, 2630)\t1\n",
      "  (0, 2852)\t1\n",
      "  (0, 4045)\t1\n",
      "  (0, 4068)\t1\n",
      "  (0, 5194)\t1\n",
      "  (0, 5565)\t1\n",
      "  (0, 6887)\t1\n",
      "  (0, 7316)\t1\n",
      "  (0, 8569)\t1\n",
      "  (0, 8780)\t1\n",
      "  (0, 9577)\t1\n",
      "  (0, 9718)\t5\n",
      "  (0, 10015)\t1\n",
      "(1, 11021)\n"
     ]
    }
   ],
   "source": [
    "#Now let's see its vector representation:\n",
    "bow4 = bow_transformer.transform([message4])\n",
    "print(bow4)\n",
    "print(bow4.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 222,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xnumberx\n",
      "°c\n"
     ]
    }
   ],
   "source": [
    "print(bow_transformer.get_feature_names()[9718])\n",
    "print(bow_transformer.get_feature_names()[10015])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#let's go and see how the bag of words counts for the entire SMS courpose in a large sparse Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages_bow = bow_transformer.transform(df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of Sparse Matrix:  (10000, 11021)\n",
      "Amount of Non-Zero occurences:  115618\n"
     ]
    }
   ],
   "source": [
    "print('Shape of Sparse Matrix: ', messages_bow.shape) #10000 row and 11021 columns\n",
    "print('Amount of Non-Zero occurences: ', messages_bow.nnz)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 223,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sparsity: 0.1049069957354142\n"
     ]
    }
   ],
   "source": [
    "sparsity = (100.0 * messages_bow.nnz / (messages_bow.shape[0] * messages_bow.shape[1]))\n",
    "print('sparsity: {}'.format(sparsity))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_bow, df['tweet_text'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: Wind 0,0 m/s ENE. Barometer 1003,9 hPa, Falling. Temperature -1,6 °C. Rain today 0,0 mm. Humidity 86%\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(bow4)[0])\n",
    "#print('expected:', df['tweet_text'].df['lable'][3])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TF-IDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  (0, 10015)\t0.1851684422417155\n",
      "  (0, 9718)\t0.7068801757366929\n",
      "  (0, 9577)\t0.18163906347855768\n",
      "  (0, 8780)\t0.17546546844461092\n",
      "  (0, 8569)\t0.18375098260352138\n",
      "  (0, 7316)\t0.15573398314335613\n",
      "  (0, 6887)\t0.18355126951106362\n",
      "  (0, 5565)\t0.1856314634878939\n",
      "  (0, 5194)\t0.16960745882552855\n",
      "  (0, 4068)\t0.18271003103682512\n",
      "  (0, 4045)\t0.18625459496884117\n",
      "  (0, 2852)\t0.22209535719935655\n",
      "  (0, 2630)\t0.3092110471050415\n",
      "  (0, 725)\t0.18521970802834364\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "\n",
    "tfidf_transformer = TfidfTransformer().fit(messages_bow)\n",
    "tfidf4 = tfidf_transformer.transform(bow4)\n",
    "print(tfidf4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.42222880912944\n",
      "3.1120647283857292\n"
     ]
    }
   ],
   "source": [
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['xnumberx']])\n",
    "print(tfidf_transformer.idf_[bow_transformer.vocabulary_['wind']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(10000, 11021)\n"
     ]
    }
   ],
   "source": [
    "messages_tfidf = tfidf_transformer.transform(messages_bow)\n",
    "print(messages_tfidf.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Wind 0,0 m/s ENE. Barometer 1003,9 hPa, Falling. Temperature -1,6 °C. Rain today 0,0 mm. Humidity 86%'\n",
      " \"@vagabondartist @JOutred @IndianaJedi I didn't want to love it. I just did. ;)\"\n",
      " '\"Happy New Year\" HAPPY NEW YEAR! 2017 is gonna be a good year indeed!! Let me wish every body a litter bit of happiness!❤️💙🇸🇴🙋\\u200d♂️'\n",
      " ...\n",
      " 'I\\'ve been playing Daily Coins in  \"Coins &amp; Coupons\" . Come and join me and win up to US $12 in coupons on the app.\\n https://t.co/Xsa5SyAHs0'\n",
      " 'Simple lunch! Roast beef steak minute with ratatouille and a classy, perfectly drinkable, pure,… https://t.co/tt0lxE4MoJ'\n",
      " 'BUT SHE LOVES LEXA\\nBUT SHE LOVES LEXA \\nBUT SHE LOVES LEXA  AND ALWAYS WILL https://t.co/Kb61OlXSRX']\n"
     ]
    }
   ],
   "source": [
    "all_predictions = spam_detect_model.predict(messages_tfidf)\n",
    "print(all_predictions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training a model TFIDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 244,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: ['I\\'ve been playing Daily Coins in  \"Coins &amp; Coupons\" . Come and join me and win up to US $12 in coupons on the app.\\n https://t.co/Xsa5SyAHs0']\n"
     ]
    }
   ],
   "source": [
    "from sklearn.naive_bayes import MultinomialNB\n",
    "spam_detect_model = MultinomialNB().fit(messages_tfidf, df['tweet_text'])\n",
    "print('predicted:', spam_detect_model.predict(tfidf4))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 243,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "predicted: I've been playing Daily Coins in  \"Coins &amp; Coupons\" . Come and join me and win up to US $12 in coupons on the app.\n",
      " https://t.co/Xsa5SyAHs0\n",
      "expected: Wind 1,4 m/s ENE. Barometer 1003,2 hPa, Falling. Temperature 7,6 °C. Rain today 0,0 mm. Humidity 99%\n"
     ]
    }
   ],
   "source": [
    "print('predicted:', spam_detect_model.predict(tfidf4)[0])\n",
    "print('expected:', df['tweet_text'].loc[815331870015627265])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [],
   "source": [
    "import string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "def text_process(mess):\n",
    "    \"\"\"\n",
    "    Takes in a string of text, then performs the following:\n",
    "    1. Remove all punctuation\n",
    "    2. Remove all stopwords\n",
    "    3. Returns a list of the cleaned text\n",
    "    \"\"\"\n",
    "    # Check characters to see if they are in punctuation\n",
    "    nopunc = [char for char in mess if char not in string.punctuation]\n",
    "\n",
    "    # Join the characters again to form the string.\n",
    "    nopunc = ''.join(nopunc)\n",
    "    \n",
    "    # Now just remove any stopwords\n",
    "    return [word for word in nopunc.split() if word.lower() not in stopwords.words('english')]\n",
    "    \n",
    "    # replace number by xnumberx\n",
    "    return [word for word in nopunc.split() if word.start not in stopwords.words('english')]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tweet_id\n",
       "815331870015627265    [Wind, 14, ms, ENE, Barometer, 10032, hPa, Fal...\n",
       "815332374909222912                                         [pasdcheval]\n",
       "815332387496357889         [Happy, New, Year, 2017, httpstcoUFob5hzO5o]\n",
       "815332527254732800                           [Happy, new, Year, sweden]\n",
       "815332597886840834                               [Happy, New, Years, 💫]\n",
       "815333076070965249      [nelson02051, Arianna483, hunk, papi, nowadays]\n",
       "815333369751990276                      [ThatManMatt, Happy, new, year]\n",
       "815333438374998016    [sarahchang, Happy, New, Year, happy, 300, yea...\n",
       "815333549289209856    [fireworks, going, since, 9pm, going, 360, aro...\n",
       "815333756932333569    [know, probably, didnt, come, January, dont, k...\n",
       "Name: tweet_text, dtype: object"
      ]
     },
     "execution_count": 127,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Check to make sure its working\n",
    "df['tweet_text'].head(10).apply(text_process)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "29846\n"
     ]
    }
   ],
   "source": [
    "# Might take awhile...\n",
    "bow_transformer = CountVectorizer(analyzer=text_process).fit(df['tweet_text'])\n",
    "\n",
    "# Print total number of vocab words\n",
    "print(len(bow_transformer.vocabulary_))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "metadata": {},
   "outputs": [],
   "source": [
    "# DF TO EXCEL\n",
    "\n",
    "writer = ExcelWriter('PythonWithClean.xlsx')\n",
    "df['tweet_text'].head(10000).apply(clean_tweet).apply(list).to_excel(writer,'Sheet1')\n",
    "writer.save()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 192,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report,confusion_matrix\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.model_selection import KFold\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import cross_val_predict\n",
    "from sklearn.metrics import f1_score, make_scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 193,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df_final['tweet_text']\n",
    "y = df_final['lable']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10000,)"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes classifier (NB)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pipeline can be used to chain multiple estimators into one.\n",
    "#This is useful as there is often a fixed sequence of steps in processing the data, \n",
    "#for example feature selection, normalization and classification. Pipeline serves multiple purposes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_tweet)),  \n",
    "    ('classifier', MultinomialNB()),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 170,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9044896755162242\n"
     ]
    }
   ],
   "source": [
    "recall = cross_val_score(pipeline, X, y, cv=10,scoring='recall')\n",
    "print(recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 171,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9746900234634277\n"
     ]
    }
   ],
   "source": [
    "precision = cross_val_score(pipeline, X, y, cv=10,scoring='precision')\n",
    "print(precision.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.92682927 0.95535714 0.94061758 0.91970803 0.8956743  0.93658537\n",
      " 0.95296524 0.93995859 0.95010846 0.9580574 ]\n"
     ]
    }
   ],
   "source": [
    "f1_score = cross_val_score(pipeline, X, y, cv=KFold(n_splits=10, shuffle=False,random_state=3), scoring=make_scorer(f1_score))\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 197,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[7698   51]\n",
      " [ 215 2036]]\n"
     ]
    }
   ],
   "source": [
    "# Confusion matrix is not a score, it is a kind of summary of what happened during evaluation.\n",
    "\n",
    "y_pred = cross_val_predict(pipeline, X , y, cv=KFold(n_splits=10, shuffle=False,random_state=3))\n",
    "conf_mat = confusion_matrix(y, y_pred)\n",
    "print(conf_mat)\n",
    "\n",
    "#[row, column]\n",
    "TP = conf_mat[1, 1]\n",
    "TN = conf_mat[0, 0]\n",
    "FP = conf_mat[0, 1]\n",
    "FN = conf_mat[1, 0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machine(SVM)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 224,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_tweet)),  \n",
    "    ('classifier', SVC(gamma='scale')),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 225,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8876086529006884\n"
     ]
    }
   ],
   "source": [
    "recall = cross_val_score(pipeline, X, y, cv=10,scoring='recall')\n",
    "print(recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 226,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9974923857868021\n"
     ]
    }
   ],
   "source": [
    "precision = cross_val_score(pipeline, X, y, cv=10,scoring='precision')\n",
    "print(precision.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'numpy.ndarray' object has no attribute '__name__'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-227-7169eb1d5a3c>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mf1_score\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcross_val_score\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpipeline\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mX\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mKFold\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mn_splits\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrandom_state\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m3\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmake_scorer\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mf1_score\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_val_score\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, error_score)\u001b[0m\n\u001b[0;32m    400\u001b[0m                                 \u001b[0mfit_params\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mfit_params\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    401\u001b[0m                                 \u001b[0mpre_dispatch\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mpre_dispatch\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 402\u001b[1;33m                                 error_score=error_score)\n\u001b[0m\u001b[0;32m    403\u001b[0m     \u001b[1;32mreturn\u001b[0m \u001b[0mcv_results\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'test_score'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    404\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\model_selection\\_validation.py\u001b[0m in \u001b[0;36mcross_validate\u001b[1;34m(estimator, X, y, groups, scoring, cv, n_jobs, verbose, fit_params, pre_dispatch, return_train_score, return_estimator, error_score)\u001b[0m\n\u001b[0;32m    226\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    227\u001b[0m     \u001b[0mcv\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mcheck_cv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0my\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mclassifier\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mis_classifier\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 228\u001b[1;33m     \u001b[0mscorers\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0m_\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0m_check_multimetric_scoring\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    229\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    230\u001b[0m     \u001b[1;31m# We clone the estimator to make sure that all the folds are\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m_check_multimetric_scoring\u001b[1;34m(estimator, scoring)\u001b[0m\n\u001b[0;32m    349\u001b[0m                            \u001b[1;34m\"mapped to the callable for multiple metric \"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    350\u001b[0m                            \u001b[1;34m\"evaluation. Got %s of type %s\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 351\u001b[1;33m                            % (repr(scoring), type(scoring)))\n\u001b[0m\u001b[0;32m    352\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    353\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mscoring\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;33m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32m~\\AppData\\Local\\Continuum\\anaconda3\\lib\\site-packages\\sklearn\\metrics\\scorer.py\u001b[0m in \u001b[0;36m__repr__\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     55\u001b[0m                                  for k, v in self._kwargs.items()])\n\u001b[0;32m     56\u001b[0m         return (\"make_scorer(%s%s%s%s)\"\n\u001b[1;32m---> 57\u001b[1;33m                 % (self._score_func.__name__,\n\u001b[0m\u001b[0;32m     58\u001b[0m                    \u001b[1;34m\"\"\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_sign\u001b[0m \u001b[1;33m>\u001b[0m \u001b[1;36m0\u001b[0m \u001b[1;32melse\u001b[0m \u001b[1;34m\", greater_is_better=False\"\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     59\u001b[0m                    self._factory_args(), kwargs_string))\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'numpy.ndarray' object has no attribute '__name__'"
     ]
    }
   ],
   "source": [
    "f1_score = cross_val_score(pipeline, X, y, cv=KFold(n_splits=10, shuffle=False,random_state=3), scoring=make_scorer(f1_score))\n",
    "print(f1_score)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Random Forest(RF)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline([\n",
    "    ('bow', CountVectorizer(analyzer=clean_tweet)),  \n",
    "    ('classifier', RandomForestClassifier(n_estimators=600)),  \n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9662340216322518\n"
     ]
    }
   ],
   "source": [
    "recall = cross_val_score(pipeline, X, y, cv=10,scoring='recall')\n",
    "print(recall.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.9986188136188137\n"
     ]
    }
   ],
   "source": [
    "precision = cross_val_score(pipeline, X, y, cv=10,scoring='precision')\n",
    "print(precision.mean())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
